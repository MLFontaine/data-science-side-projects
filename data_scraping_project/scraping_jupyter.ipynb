{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping web data for use with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scrapy module for data scraping, see: https://doc.scrapy.org/en/1.4/intro/overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://adequateman.deadspin.com/whats-the-best-store-to-daydream-about-robbing-1796232076\n",
    "http://deadspin.com/tag/funbag\n",
    "\n",
    "IDEA 1: maybe have the scrapyspider collect the urls, and then run through beautiful soup like below and write each one to its own text file. Perhaps this is formally two processes, web crawling with spider, and then html parsing and cleanup for each page\n",
    "\n",
    "there's also some type of querying language with an x in it to use... XPATH\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "IDEA 2: For the projects... one version reproduces whole column. another version you enter a question and it generates an answer for you... would be interesting to do for an \"agony aunt\" column... should be a lot of data to scrape there...\n",
    "\n",
    "TO DO: \n",
    "- Fix up formatting so apostrophes don't get messed up in html (.prettify(formatter=\"html\")) !!!!!\n",
    "- Figure out how to not repeat the contents of blockquote\n",
    "- Crawl the site to make a file of all the urls. Print that to text file, so it can be loaded in and looped into bs4 stuff below\n",
    "\n",
    "IDEA 3: scrapy might be good for formatting Q and A json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the URLS for each html page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spider and run it from shell. Porbably a nicer way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "#top_url = 'http://deadspin.com/tag/funbag'\n",
    "\n",
    "with open('funbag_spider.py', 'w') as swf:\n",
    "    swf.write('''\\\n",
    "import scrapy\n",
    "\n",
    "class FunbagSpider(scrapy.Spider):\n",
    "    name = 'funbag_urls'\n",
    "    start_urls = ['http://deadspin.com/tag/funbag']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for entry in response.css('h1.entry-title'):\n",
    "            yield {\n",
    "                'url': entry.css('a::attr(href)').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('div.load-more__button a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page_url = 'http://deadspin.com/tag/funbag' + next_page\n",
    "            yield scrapy.Request(next_page_url)\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scrapy runspider funbag_spider.py -o funbag_urls.json &>/dev/null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load urls back in from json... can do nicer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://adequateman.deadspin.com/whats-the-best-store-to-daydream-about-robbing-1796232076', 'http://adequateman.deadspin.com/should-you-ask-people-their-politics-before-dating-them-1796052163', 'http://adequateman.deadspin.com/is-an-unbeaten-playoff-run-more-impressive-than-73-wins-1795847201']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('funbag_urls.json') as url_file:    \n",
    "    url_data = json.load(url_file)\n",
    "\n",
    "all_urls = []\n",
    "for place in url_data:\n",
    "    all_urls.append(str(place.values()[0]))\n",
    "\n",
    "print all_urls[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse, sanitize, and save each of the html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u01a1' in position 254: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-d96416e77524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m432\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmy_outfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.html'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moneFullPageParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_outfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-218-d96416e77524>\u001b[0m in \u001b[0;36moneFullPageParse\u001b[0;34m(url, outfile)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'blockquote'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#anchor.prettify() #anchor.contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u01a1' in position 254: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "def oneFullPageParse(url, outfile):\n",
    "    os.chdir(os.getcwd()+'/full_page_files')\n",
    "    fo = open(outfile, 'w')\n",
    "    funbag = urllib2.urlopen(url)\n",
    "    #funbaged = funbag.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "    soup = BeautifulSoup(funbag,'html5lib') #html.parser') #'html5lib'\n",
    "    for anchor in soup.find_all(['p', 'blockquote'], class_ = ''): \n",
    "        if anchor.parent.name == 'blockquote':\n",
    "            continue\n",
    "        fo.write(anchor.prettify(formatter=\"html\")) #anchor.prettify() #anchor.contents\n",
    "    fo.close()\n",
    "    os.chdir(os.getcwd()+'/..')\n",
    "    \n",
    "###\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#loop through all urls to generate cleaned up pages\n",
    "for idx, row in enumerate(all_urls[7:432]):\n",
    "    my_outfile = str(idx) + '.html'\n",
    "    oneFullPageParse(row, my_outfile)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print 'Total time:', total_time\n",
    "\n",
    "#my_url = all_urls[0]\n",
    "#'http://adequateman.deadspin.com/whats-the-best-store-to-daydream-about-robbing-1796232076'   \n",
    "#oneFullPageParse(my_url, my_outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2.944/2)*431/60\n",
    "#os.getcwd()\n",
    "#os.chdir(os.getcwd()+'/..') #need to fix the handling of directories\n",
    "#os.getcwd()\n",
    "#print len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Junked code\n",
    "\n",
    "#from HTMLParser import HTMLParser\n",
    "#parser = HTMLParser()\n",
    "#parser.feed('https://docs.python.org/2/library/htmlparser.html')\n",
    "\n",
    "#------\n",
    "#%lsmagic\n",
    "#!ps -a\n",
    "##%time\n",
    "##2+2\n",
    "#------\n",
    "    #'p', class_ = '' #removes the p tags that have a class attribute!!! just need blockquotes\n",
    "    #['p', 'blockquote'] # almost right, but repeats the contents of the blockquote, maybe can do something with parent/child\n",
    "    #'p' and 'blockquote' #gets the two used together... could just generate questions with this for a start\n",
    "    #'p' or 'blockquote' #doesn't print out the blockquote tag but has the data...\n",
    "    #print anchor.parent #do something with this, it works! find parent though?!\n",
    "    \n",
    "    #print anchor.previousSibling\n",
    "    ###print soup.find_parent('anchor')\n",
    "    \n",
    "    #maybe have something like if parent of p is block quote, do nothing <<<<<<<<<<<<\n",
    "    \n",
    "#looks like we want to strip out certain classes for p, like the ad label. maybe do with text processing outside bs4\n",
    "#do that, and throw in the blockquotes, and bob's your uncle?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
