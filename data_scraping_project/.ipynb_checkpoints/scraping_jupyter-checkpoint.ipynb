{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping web data for use with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/MF/Desktop/data-science-side-projects/data_scraping_project'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()+'/stuff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scrapy module for data scraping, see: https://doc.scrapy.org/en/1.4/intro/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('quotes_spider.py', 'w') as swf:\n",
    "    swf.write('''\\\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/tag/humor/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.xpath('span/small/text()').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').extract_first()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "            ''')\n",
    "    swf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! scrapy runspider quotes_spider.py -o quotes.json &>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://adequateman.deadspin.com/whats-the-best-store-to-daydream-about-robbing-1796232076\n",
    "http://deadspin.com/tag/funbag\n",
    "\n",
    "IDEA 1: maybe have the scrapyspider collect the urls, and then run through beautiful soup like below and write each one to its own text file. Perhaps this is formally two processes, web crawling with spider, and then html parsing and cleanup for each page\n",
    "\n",
    "there's also some type of querying language with an x in it to use... XPATH\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "IDEA 2: For the projects... one version reproduces whole column. another version you enter a question and it generates an answer for you... would be interesting to do for an \"agony aunt\" column... should be a lot of data to scrape there...\n",
    "\n",
    "TO DO: \n",
    "- Fix up formatting so apostrophes don't get messed up in html (.prettify(formatter=\"html\")) !!!!!\n",
    "- Figure out how to not repeat the contents of blockquote\n",
    "- Crawl the site to make a file of all the urls. Print that to text file, so it can be loaded in and looped into bs4 stuff below\n",
    "\n",
    "IDEA 3: scrapy might be good for formatting Q and A json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the URLS for each html page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spider and run it from shell. Porbably a nicer way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "#top_url = 'http://deadspin.com/tag/funbag'\n",
    "\n",
    "with open('funbag_spider.py', 'w') as swf:\n",
    "    swf.write('''\\\n",
    "import scrapy\n",
    "\n",
    "class FunbagSpider(scrapy.Spider):\n",
    "    name = 'funbag_urls'\n",
    "    start_urls = ['http://deadspin.com/tag/funbag']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for entry in response.css('h1.entry-title'):\n",
    "            yield {\n",
    "                'url': entry.css('a::attr(href)').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('div.load-more__button a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page_url = 'http://deadspin.com/tag/funbag' + next_page\n",
    "            yield scrapy.Request(next_page_url)\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scrapy runspider funbag_spider.py -o funbag_urls.json &>/dev/null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load urls back in from json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse, sanitize, and save each of the html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "my_url = 'http://adequateman.deadspin.com/whats-the-best-store-to-daydream-about-robbing-1796232076'\n",
    "my_outfile = 'first.html'\n",
    "\n",
    "def oneFullPageParse(url, outfile):\n",
    "    fo = open(outfile, 'w')\n",
    "    funbag = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(funbag,'html5lib') #html.parser')\n",
    "    for anchor in soup.find_all(['p', 'blockquote'], class_ = ''): \n",
    "        if anchor.parent.name == 'blockquote':\n",
    "            continue\n",
    "        fo.write(anchor.prettify(formatter=\"html\")) #anchor.prettify() #anchor.contents\n",
    "    fo.close()\n",
    "    \n",
    "oneFullPageParse(my_url, my_outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Junked code\n",
    "\n",
    "#from HTMLParser import HTMLParser\n",
    "#parser = HTMLParser()\n",
    "#parser.feed('https://docs.python.org/2/library/htmlparser.html')\n",
    "\n",
    "#------\n",
    "#%lsmagic\n",
    "#!ps -a\n",
    "##%time\n",
    "##2+2\n",
    "#------\n",
    "    #'p', class_ = '' #removes the p tags that have a class attribute!!! just need blockquotes\n",
    "    #['p', 'blockquote'] # almost right, but repeats the contents of the blockquote, maybe can do something with parent/child\n",
    "    #'p' and 'blockquote' #gets the two used together... could just generate questions with this for a start\n",
    "    #'p' or 'blockquote' #doesn't print out the blockquote tag but has the data...\n",
    "    #print anchor.parent #do something with this, it works! find parent though?!\n",
    "    \n",
    "    #print anchor.previousSibling\n",
    "    ###print soup.find_parent('anchor')\n",
    "    \n",
    "    #maybe have something like if parent of p is block quote, do nothing <<<<<<<<<<<<\n",
    "    \n",
    "#looks like we want to strip out certain classes for p, like the ad label. maybe do with text processing outside bs4\n",
    "#do that, and throw in the blockquotes, and bob's your uncle?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
