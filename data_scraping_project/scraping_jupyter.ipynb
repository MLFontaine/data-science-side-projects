{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping web data for use with a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Crawl this site for all articles: http://deadspin.com/tag/funbag\n",
    "- Step 2: Parse just the text content, with some html tagging into, into individual html files\n",
    "- Step 3: Concatenate into one large file for feeding into the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using scrapy module for data scraping, see: https://doc.scrapy.org/en/1.4/intro/overview.html\n",
    "\n",
    "IDEA 1: maybe have the scrapyspider collect the urls, and then run through beautiful soup like below and write each one to its own text file. Perhaps this is formally two processes, web crawling with spider, and then html parsing and cleanup for each page\n",
    "\n",
    "there's also some type of querying language with an x in it to use... XPATH\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "IDEA 2: For the projects... one version reproduces whole column. another version you enter a question and it generates an answer for you... would be interesting to do for an \"agony aunt\" column... should be a lot of data to scrape there...\n",
    "\n",
    "TO DO: \n",
    "- Fix up formatting so apostrophes don't get messed up in html (.prettify(formatter=\"html\")) !!!!!\n",
    "- Figure out how to not repeat the contents of blockquote\n",
    "- Crawl the site to make a file of all the urls. Print that to text file, so it can be loaded in and looped into bs4 stuff below\n",
    "\n",
    "IDEA 3: scrapy might be good for formatting Q and A json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the URLS for each html page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spider and run it from shell. Probably a nicer way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "#top_url = 'http://deadspin.com/tag/funbag'\n",
    "\n",
    "with open('funbag_spider.py', 'w') as swf:\n",
    "    swf.write('''\\\n",
    "import scrapy\n",
    "\n",
    "class FunbagSpider(scrapy.Spider):\n",
    "    name = 'funbag_urls'\n",
    "    start_urls = ['http://deadspin.com/tag/funbag']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for entry in response.css('h1.entry-title'):\n",
    "            yield {\n",
    "                'url': entry.css('a::attr(href)').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('div.load-more__button a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page_url = 'http://deadspin.com/tag/funbag' + next_page\n",
    "            yield scrapy.Request(next_page_url)\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! scrapy runspider funbag_spider.py -o funbag_urls.json &>/dev/null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load urls back in from json... could do without intermediate text files probably..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://adequateman.deadspin.com/whats-the-best-store-to-daydream-about-robbing-1796232076', 'http://adequateman.deadspin.com/should-you-ask-people-their-politics-before-dating-them-1796052163', 'http://adequateman.deadspin.com/is-an-unbeaten-playoff-run-more-impressive-than-73-wins-1795847201']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('funbag_urls.json') as url_file:    \n",
    "    url_data = json.load(url_file)\n",
    "\n",
    "all_urls = []\n",
    "for place in url_data:\n",
    "    all_urls.append(str(place.values()[0]))\n",
    "\n",
    "print all_urls[0:3]\n",
    "#might want to reverse the list to start at the oldest one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse, sanitize, and save each of the html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io #to fix a problem with writing unicode it seems\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "def oneFullPageParse(url, outfile):\n",
    "    fo = io.open(os.path.join(os.getcwd(), 'full_page_files', outfile), 'w', encoding = 'utf-8')\n",
    "    funbag = urllib2.urlopen(url)\n",
    "    #funbaged = funbag.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "    soup = BeautifulSoup(funbag,'html5lib') #html.parser') #'html5lib'\n",
    "    for anchor in soup.find_all(['p', 'blockquote'], class_ = ''): \n",
    "        if anchor.parent.name == 'blockquote': #prevents repeating the <p> tags inside\n",
    "            continue\n",
    "        fo.write(anchor.prettify(formatter=\"html\")) #print anchor.prettify(formatter=\"html\") #anchor.prettify() #anchor.contents\n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 586.253000021\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#loop through all urls to generate cleaned up pages\n",
    "for idx, row in enumerate(all_urls):\n",
    "    my_outfile = str(idx) + '.html'\n",
    "    oneFullPageParse(row, my_outfile)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print 'Total time:', total_time\n",
    "#got Total time: 586.253000021, 9.7 minutes, when going through everything on old Windows desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fontainem\\\\Desktop\\\\Data_Science\\\\data-science-side-projects\\\\data_scraping_project'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir(os.getcwd()+'/..')\n",
    "#os.getcwd()\n",
    "#print os.path.join(os.getcwd(),'full_page_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train character level language model using LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://github.com/karpathy/char-rnn\n",
    "\n",
    "https://github.com/jcjohnson/torch-rnn\n",
    "\n",
    "This last one is the best one to use...\n",
    "Here's an installation guide for OSX\n",
    "\n",
    "http://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/\n",
    "\n",
    "it appears a lot of people have this issue with the HDF5 library. You can see this solution, which should work: http://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/comment-page-1/#comment-275266."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Junked code\n",
    "\n",
    "#from HTMLParser import HTMLParser\n",
    "#parser = HTMLParser()\n",
    "#parser.feed('https://docs.python.org/2/library/htmlparser.html')\n",
    "\n",
    "#------\n",
    "#%lsmagic\n",
    "#!ps -a\n",
    "##%time\n",
    "##2+2\n",
    "#------\n",
    "    #'p', class_ = '' #removes the p tags that have a class attribute!!! just need blockquotes\n",
    "    #['p', 'blockquote'] # almost right, but repeats the contents of the blockquote, maybe can do something with parent/child\n",
    "    #'p' and 'blockquote' #gets the two used together... could just generate questions with this for a start\n",
    "    #'p' or 'blockquote' #doesn't print out the blockquote tag but has the data...\n",
    "    #print anchor.parent #do something with this, it works! find parent though?!\n",
    "    \n",
    "    #print anchor.previousSibling\n",
    "    ###print soup.find_parent('anchor')\n",
    "    \n",
    "    #maybe have something like if parent of p is block quote, do nothing <<<<<<<<<<<<\n",
    "    \n",
    "#looks like we want to strip out certain classes for p, like the ad label. maybe do with text processing outside bs4\n",
    "#do that, and throw in the blockquotes, and bob's your uncle?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
